def predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False):
    "Generates output predictions for the input samples.\n\n        Computation is done in batches.\n\n        # Arguments\n            x: Input data. It could be:\n                - A Numpy array (or array-like), or a list of arrays\n                  (in case the model has multiple inputs).\n                - A dict mapping input names to the corresponding\n                  array/tensors, if the model has named inputs.\n                - A generator or `keras.utils.Sequence` returning\n                  `(inputs, targets)` or `(inputs, targets, sample weights)`.\n                - None (default) if feeding from framework-native\n                  tensors (e.g. TensorFlow data tensors).\n            batch_size: Integer or `None`.\n                Number of samples per gradient update.\n                If unspecified, `batch_size` will default to 32.\n                Do not specify the `batch_size` if your data is in the\n                form of symbolic tensors, generators, or\n                `keras.utils.Sequence` instances (since they generate batches).\n            verbose: Verbosity mode, 0 or 1.\n            steps: Total number of steps (batches of samples)\n                before declaring the prediction round finished.\n                Ignored with the default value of `None`.\n            callbacks: List of `keras.callbacks.Callback` instances.\n                List of callbacks to apply during prediction.\n                See [callbacks](/callbacks).\n            max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n                input only. Maximum size for the generator queue.\n                If unspecified, `max_queue_size` will default to 10.\n            workers: Integer. Used for generator or `keras.utils.Sequence` input\n                only. Maximum number of processes to spin up when using\n                process-based threading. If unspecified, `workers` will default\n                to 1. If 0, will execute the generator on the main thread.\n            use_multiprocessing: Boolean. Used for generator or\n                `keras.utils.Sequence` input only. If `True`, use process-based\n                threading. If unspecified, `use_multiprocessing` will default to\n                `False`. Note that because this implementation relies on\n                multiprocessing, you should not pass non-picklable arguments to\n                the generator as they can't be passed easily to children processes.\n\n        # Returns\n            Numpy array(s) of predictions.\n\n        # Raises\n            ValueError: In case of mismatch between the provided\n                input data and the model's expectations,\n                or in case a stateful model receives a number of samples\n                that is not a multiple of the batch size.\n        "
    batch_size = self._validate_or_infer_batch_size(batch_size, steps, x)
    if training_utils.is_generator_or_sequence(x):
        return self.predict_generator(x, steps=steps, verbose=verbose, callbacks=callbacks, max_queue_size=max_queue_size, workers=workers, use_multiprocessing=use_multiprocessing)
    if ((x is None) and (steps is None)):
        raise ValueError('If predicting from data tensors, you should specify the `steps` argument.')
    (x, _, _) = self._standardize_user_data(x)
    if self.stateful:
        if ((x[0].shape[0] > batch_size) and ((x[0].shape[0] % batch_size) != 0)):
            raise ValueError((((('In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: ' + str(x[0].shape[0])) + ' samples. Batch size: ') + str(batch_size)) + '.'))
    if self._uses_dynamic_learning_phase():
        ins = (x + [0])
    else:
        ins = x
    self._make_predict_function()
    f = self.predict_function
    return training_arrays.predict_loop(self, f, ins, batch_size=batch_size, verbose=verbose, steps=steps, callbacks=callbacks)