def cross_entropy(dist1, dist2):
    'Computes Cross entropy.\n\n    For two continuous distributions :math:`p(x), q(x)`, it is expressed as\n\n    .. math::\n        H(p,q) = - \\int p(x) \\log q(x) dx\n\n    For two discrete distributions :math:`p(x), q(x)`, it is expressed as\n\n    .. math::\n        H(p,q) = - \\sum_x p(x) \\log q(x)\n\n    This function call `kl_divergence` and `entropy` of `dist1`. Therefore,\n    it is necessary to register KL divergence function with `register_kl`\n    decoartor and define `entropy` in `dist1`.\n\n    Args:\n        dist1(:class:`~chainer.Distribution`): Distribution to calculate cross\n            entropy :math:`p`. This is the first (left) operand of the cross\n            entropy.\n        dist2(:class:`~chainer.Distribution`): Distribution to calculate cross\n            entropy :math:`q`. This is the second (right) operand of the cross\n            entropy.\n\n    Returns:\n        ~chainer.Variable: Output variable representing cross entropy\n        :math:`H(p,q)`.\n\n    '
    return (dist1.entropy() + kl_divergence(dist1, dist2))