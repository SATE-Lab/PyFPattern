@wrap_name_default('gru_group')
def gru_group(input, memory_boot=None, size=None, name=None, reverse=False, gru_bias_attr=None, gru_param_attr=None, act=None, gate_act=None, gru_layer_attr=None, naive=False):
    '\n    gru_group is a recurrent_group version of Gated Recurrent Unit. It\n    does exactly the same calculation as the grumemory layer does. A promising\n    benefit is that gru hidden states are accessible to the user. This is\n    especially useful in attention model. If you do not need to access\n    any internal state and merely use the outputs of a GRU, it is recommended\n    to use the grumemory, which is relatively faster.\n\n    Please see grumemory in layers.py for more detail about the maths.\n\n    The example usage is:\n\n    ..  code-block:: python\n\n        gru = gru_group(input=[layer1],\n                        size=256,\n                        act=TanhActivation(),\n                        gate_act=SigmoidActivation())\n\n    :param input: input layer.\n    :type input: LayerOutput\n    :param memory_boot: the initialization state of the LSTM cell.\n    :type memory_boot: LayerOutput | None\n    :param name: name of the gru group.\n    :type name: basestring\n    :param size: hidden size of the gru.\n    :type size: int\n    :param reverse: process the input in a reverse order or not.\n    :type reverse: bool\n    :param act: activiation type of gru\n    :type act: BaseActivation\n    :param gate_act: gate activiation type of gru\n    :type gate_act: BaseActivation\n    :param gru_bias_attr: bias parameter attribute of gru layer,\n                          False means no bias, None means default bias.\n    :type gru_bias_attr: ParameterAttribute|False|None\n    :param gru_layer_attr: Extra attribute of the gru layer.\n    :type gru_layer_attr: ExtraLayerAttribute\n    :return: the gru group.\n    :rtype: LayerOutput\n    '

    def __gru_step__(ipt):
        return gru_unit(input=ipt, memory_boot=memory_boot, name=name, size=size, gru_bias_attr=gru_bias_attr, gru_param_attr=gru_param_attr, act=act, gate_act=gate_act, gru_layer_attr=gru_layer_attr, naive=naive)
    return recurrent_group(name=('%s_recurrent_group' % name), step=__gru_step__, reverse=reverse, input=input)