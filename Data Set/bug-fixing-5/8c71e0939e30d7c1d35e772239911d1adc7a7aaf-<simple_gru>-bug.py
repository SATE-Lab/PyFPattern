@wrap_name_default('simple_gru')
def simple_gru(input, size, name=None, reverse=False, mixed_param_attr=None, mixed_bias_param_attr=None, mixed_layer_attr=None, gru_bias_attr=None, gru_param_attr=None, act=None, gate_act=None, gru_layer_attr=None, naive=False):
    '\n    You maybe see gru_step_layer, grumemory in layers.py, gru_unit, gru_group,\n    simple_gru in network.py. The reason why there are so many interfaces is\n    that we have two ways to implement recurrent neural network. One way is to\n    use one complete layer to implement rnn (including simple rnn, gru and lstm)\n    with multiple time steps, such as recurrent_layer, lstmemory, grumemory. But,\n    the multiplication operation :math:`W x_t` is not computed in these layers.\n    See details in their interfaces in layers.py.\n    The other implementation is to use an recurrent group which can ensemble a\n    series of layers to compute rnn step by step. This way is flexible for\n    attenion mechanism or other complex connections.\n\n    - gru_step_layer: only compute rnn by one step. It needs an memory as input\n      and can be used in recurrent group.\n    - gru_unit: a wrapper of gru_step_layer with memory.\n    - gru_group: a GRU cell implemented by a combination of multiple layers in\n      recurrent group.\n      But :math:`W x_t` is not done in group.\n    - gru_memory: a GRU cell implemented by one layer, which does same calculation\n      with gru_group and is faster than gru_group.\n    - simple_gru: a complete GRU implementation inlcuding :math:`W x_t` and\n      gru_group. :math:`W` contains :math:`W_r`, :math:`W_z` and :math:`W`, see\n      formula in grumemory.\n\n    The computational speed is that, grumemory is relatively better than\n    gru_group, and gru_group is relatively better than simple_gru.\n\n    The example usage is:\n\n    ..  code-block:: python\n\n        gru = simple_gru(input=[layer1], size=256)\n\n    :param input: input layer name.\n    :type input: LayerOutput\n    :param name: name of the gru group.\n    :type name: basestring\n    :param size: hidden size of the gru.\n    :type size: int\n    :param reverse: whether to process the input data in a reverse order\n    :type reverse: bool\n    :param act: type of the activiation\n    :type act: BaseActivation\n    :param gate_act: type of the gate activiation\n    :type gate_act: BaseActivation\n    :param gru_bias_attr: bias. False means no bias, None means default bias.\n    :type gru_bias_attr: ParameterAttribute|False\n    :param gru_layer_attr: Extra parameter attribute of the gru layer.\n    :type gru_layer_attr: ParameterAttribute|False\n    :return: the gru group.\n    :rtype: LayerOutput\n    '
    with mixed_layer(name=('%s_transform' % name), size=(size * 3), bias_attr=mixed_bias_param_attr, layer_attr=mixed_layer_attr) as m:
        m += full_matrix_projection(input=input, param_attr=mixed_param_attr)
    return gru_group(name=name, size=size, input=m, reverse=reverse, gru_bias_attr=gru_bias_attr, gru_param_attr=gru_param_attr, act=act, gate_act=gate_act, gru_layer_attr=gru_layer_attr, naive=naive)