def softmax_cross_entropy_with_logits(logits, labels, name=None):
    'Computes softmax cross entropy between `logits` and `labels`.\n\n  Measures the probability error in discrete classification tasks in which the\n  classes are mutually exclusive (each entry is in exactly one class).  For\n  example, each CIFAR-10 image is labeled with one and only one label: an image\n  can be a dog or a truck, but not both.\n\n  **NOTE:**  While the classes are mutually exclusive, their probabilities\n  need not be.  All that is required is that each row of `labels` is\n  a valid probability distribution.  If using exclusive `labels`\n  (wherein one and only one class is true at a time), see\n  `sparse_softmax_cross_entropy_with_logits`.\n\n  **WARNING:** This op expects unscaled logits, since it performs a `softmax`\n  on `logits` internally for efficiency.  Do not call this op with the\n  output of `softmax`, as it will produce incorrect results.\n\n  `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n  and the same dtype (either `float32` or `float64`).\n\n  Args:\n    logits: Unscaled log probabilities.\n    labels: Each row `labels[i]` must be a valid probability distribution.\n    name: A name for the operation (optional).\n\n  Returns:\n    A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the\n    softmax cross entropy loss.\n  '
    (cost, unused_backprop) = gen_nn_ops._softmax_cross_entropy_with_logits(logits, labels, name=name)
    return cost