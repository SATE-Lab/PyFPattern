def append_backward(loss, parameter_list=None, no_grad_set=None, callbacks=None):
    '\n    Append backward part to main_program\n\n    Args:\n        loss(Variable): The variable generated by cost function.\n        parameter_list(list[string]): Parameters that need to be updated by\n            optimizer. If None, it means all parameters need to be updated.\n        no_grad_set(set): Variables that have no gradients in Block 0.\n            All variables with `step_gradient=True` from all blocks will be\n            automatically added.\n\n    Return:\n        (list[(Variable,Variable)]): list of (parameter, gradient) pair.\n    '
    assert isinstance(loss, framework.Variable)
    if (callbacks is not None):
        isinstance(callbacks, list)
    program = loss.block.program
    if (no_grad_set is None):
        no_grad_set = set()
    no_grad_set = copy.copy(no_grad_set)
    no_grad_dict = _get_stop_gradients_(program)
    no_grad_dict[0].update(map(_append_grad_suffix_, no_grad_set))
    grad_info_map = dict()
    root_block = program.block(0)
    fwd_op_num = root_block.desc.op_size()
    current_block_idx = program.current_block_idx
    grad_to_var = dict()
    op_desc = _create_op_desc_('fill_constant', {
        
    }, {
        'Out': [_append_grad_suffix_(loss.name)],
    }, {
        'shape': [1],
        'value': 1.0,
        'dtype': loss.dtype,
        'force_cpu': False,
    })
    root_block.desc.append_op().copy_from(op_desc)
    block_no_grad_set = set(map(_strip_grad_suffix_, no_grad_dict[0]))
    op_path = _find_op_path_(root_block, [loss], [], block_no_grad_set)
    no_grad_dict[0].update(map(_append_grad_suffix_, block_no_grad_set))
    _append_backward_ops_(root_block, op_path, root_block, no_grad_dict, grad_to_var, callbacks)
    _rename_grad_(root_block, fwd_op_num, grad_to_var, {
        
    })
    _append_backward_vars_(root_block, fwd_op_num, grad_to_var, grad_info_map)
    program.current_block_idx = current_block_idx
    program.sync_with_cpp()
    loss.block.var(_append_grad_suffix_(loss.name)).persistable = True
    if (parameter_list is not None):
        parameters = parameter_list
    else:
        params = program.global_block().all_parameters()
        parameters = [param.name for param in params]
    params_and_grads = []
    for param in parameters:
        if (param not in grad_info_map):
            continue
        grad_info = grad_info_map[param]
        grad_block = grad_info[1]
        if (not grad_block.has_var(grad_info[0])):
            raise ValueError('grad block[{0}] did not have grad var {1}'.format(grad_info[1], grad_info[0]))
        param_var = program.global_block().var(param)
        grad_var = grad_block.var(grad_info[0])
        if loss.block.has_var(grad_info[0]):
            params_and_grads.append((param_var, grad_var))
        else:
            params_and_grads.append((param_var, None))
    return params_and_grads