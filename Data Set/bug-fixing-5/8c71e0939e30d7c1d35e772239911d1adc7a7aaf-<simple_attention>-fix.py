@wrap_name_default()
@wrap_act_default(param_names=['weight_act'], act=TanhActivation())
def simple_attention(encoded_sequence, encoded_proj, decoder_state, transform_param_attr=None, softmax_param_attr=None, weight_act=None, name=None):
    "\n    Calculate and return a context vector with attention mechanism.\n    Size of the context vector equals to size of the encoded_sequence.\n\n    ..  math::\n\n        a(s_{i-1},h_{j}) & = v_{a}f(W_{a}s_{t-1} + U_{a}h_{j})\n\n        e_{i,j} & = a(s_{i-1}, h_{j})\n\n        a_{i,j} & = \\frac{exp(e_{i,j})}{\\sum_{k=1}^{T_x}{exp(e_{i,k})}}\n\n        c_{i} & = \\sum_{j=1}^{T_{x}}a_{i,j}h_{j}\n\n    where :math:`h_{j}` is the jth element of encoded_sequence,\n    :math:`U_{a}h_{j}` is the jth element of encoded_proj\n    :math:`s_{i-1}` is decoder_state\n    :math:`f` is weight_act, and is set to tanh by default.\n\n    Please refer to **Neural Machine Translation by Jointly Learning to\n    Align and Translate** for more details. The link is as follows:\n    https://arxiv.org/abs/1409.0473.\n\n    The example usage is:\n\n    ..  code-block:: python\n\n        context = simple_attention(encoded_sequence=enc_seq,\n                                   encoded_proj=enc_proj,\n                                   decoder_state=decoder_prev,)\n\n    :param name: name of the attention model.\n    :type name: basestring\n    :param softmax_param_attr: parameter attribute of sequence softmax\n                               that is used to produce attention weight.\n    :type softmax_param_attr: ParameterAttribute\n    :param weight_act: activation of the attention model.\n    :type weight_act: BaseActivation\n    :param encoded_sequence: output of the encoder\n    :type encoded_sequence: LayerOutput\n    :param encoded_proj: attention weight is computed by a feed forward neural\n                         network which has two inputs : decoder's hidden state\n                         of previous time step and encoder's output.\n                         encoded_proj is output of the feed-forward network for\n                         encoder's output. Here we pre-compute it outside\n                         simple_attention for speed consideration.\n    :type encoded_proj: LayerOutput\n    :param decoder_state: hidden state of decoder in previous time step\n    :type decoder_state: LayerOutput\n    :param transform_param_attr: parameter attribute of the feed-forward\n                                network that takes decoder_state as inputs to\n                                compute attention weight.\n    :type transform_param_attr: ParameterAttribute\n    :return: a context vector\n    "
    assert (encoded_proj.size == decoder_state.size)
    proj_size = encoded_proj.size
    with mixed_layer(size=proj_size, name=('%s_transform' % name)) as m:
        m += full_matrix_projection(decoder_state, param_attr=transform_param_attr)
    expanded = expand_layer(input=m, expand_as=encoded_sequence, name=('%s_expand' % name))
    with mixed_layer(size=proj_size, act=weight_act, name=('%s_combine' % name)) as m:
        m += identity_projection(expanded)
        m += identity_projection(encoded_proj)
    attention_weight = fc_layer(input=m, size=1, act=SequenceSoftmaxActivation(), param_attr=softmax_param_attr, name=('%s_softmax' % name), bias_attr=False)
    scaled = scaling_layer(weight=attention_weight, input=encoded_sequence, name=('%s_scaling' % name))
    return pooling_layer(input=scaled, pooling_type=SumPooling(), name=('%s_pooling' % name))