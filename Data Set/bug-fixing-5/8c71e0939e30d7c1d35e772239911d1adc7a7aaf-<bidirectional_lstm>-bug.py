@wrap_name_default('bidirectional_lstm')
def bidirectional_lstm(input, size, name=None, return_seq=False, fwd_mat_param_attr=None, fwd_bias_param_attr=None, fwd_inner_param_attr=None, fwd_act=None, fwd_gate_act=None, fwd_state_act=None, fwd_mixed_layer_attr=None, fwd_lstm_cell_attr=None, bwd_mat_param_attr=None, bwd_bias_param_attr=None, bwd_inner_param_attr=None, bwd_act=None, bwd_gate_act=None, bwd_state_act=None, bwd_mixed_layer_attr=None, bwd_lstm_cell_attr=None, last_seq_attr=None, first_seq_attr=None, concat_attr=None, concat_act=None):
    '\n    A bidirectional_lstm is a recurrent unit that iterates over the input\n    sequence both in forward and bardward orders, and then concatenate two\n    outputs form a final output. However, concatenation of two outputs\n    is not the only way to form the final output, you can also, for example,\n    just add them together.\n\n    Please refer to  **Neural Machine Translation by Jointly Learning to Align\n    and Translate** for more details about the bidirectional lstm.\n    The link goes as follows:\n    .. _Link: https://arxiv.org/pdf/1409.0473v3.pdf\n\n    The example usage is:\n\n    ..  code-block:: python\n\n        bi_lstm = bidirectional_lstm(input=[input1], size=512)\n\n    :param name: bidirectional lstm layer name.\n    :type name: basestring\n    :param input: input layer.\n    :type input: LayerOutput\n    :param size: lstm layer size.\n    :type size: int\n    :param return_seq: If set False, outputs of the last time step are\n                       concatenated and returned.\n                       If set True, the entire output sequences that are\n                       processed in forward and backward directions are\n                       concatenated and returned.\n    :type return_seq: bool\n    :return: LayerOutput object accroding to the return_seq.\n    :rtype: LayerOutput\n    '
    args = locals()
    fw = simple_lstm(name=('%s_fw' % name), input=input, size=size, **dict(((k[len('fwd_'):], v) for (k, v) in args.iteritems() if k.startswith('fwd_'))))
    bw = simple_lstm(name=('%s_bw' % name), input=input, size=size, reverse=True, **dict(((k[len('bwd_'):], v) for (k, v) in args.iteritems() if k.startswith('bwd_'))))
    if return_seq:
        return concat_layer(name=name, input=[fw, bw], layer_attr=concat_attr, act=concat_act)
    else:
        fw_seq = last_seq(name=('%s_fw_last' % name), input=fw, layer_attr=last_seq_attr)
        bw_seq = first_seq(name=('%s_bw_last' % name), input=bw, layer_attr=first_seq_attr)
        return concat_layer(name=name, input=[fw_seq, bw_seq], layer_attr=concat_attr, act=concat_act)