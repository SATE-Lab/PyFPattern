def parse_args():
    parser = argparse.ArgumentParser(description='Train Faster-RCNN networks e2e.')
    parser.add_argument('--network', type=str, default='resnet50_v1b', help='Base network name which serves as feature extraction base.')
    parser.add_argument('--dataset', type=str, default='voc', help='Training dataset. Now support voc and coco.')
    parser.add_argument('--num-workers', '-j', dest='num_workers', type=int, default=4, help='Number of data workers, you can use larger number to accelerate data loading, if your CPU and GPUs are powerful.')
    parser.add_argument('--batch-size', type=int, default=1, help='Training mini-batch size.')
    parser.add_argument('--gpus', type=str, default='0', help='Training with GPUs, you can specify 1,3 for example.')
    parser.add_argument('--epochs', type=str, default='', help='Training epochs.')
    parser.add_argument('--resume', type=str, default='', help='Resume from previously saved parameters if not None. For example, you can resume from ./faster_rcnn_xxx_0123.params')
    parser.add_argument('--start-epoch', type=int, default=0, help='Starting epoch for resuming, default is 0 for new training.You can specify it to 100 for example to start from 100 epoch.')
    parser.add_argument('--lr', type=str, default='', help='Learning rate, default is 0.001 for voc single gpu training.')
    parser.add_argument('--lr-decay', type=float, default=0.1, help='decay rate of learning rate. default is 0.1.')
    parser.add_argument('--lr-decay-epoch', type=str, default='', help='epochs at which learning rate decays. default is 14,20 for voc.')
    parser.add_argument('--lr-warmup', type=str, default='', help='warmup iterations to adjust learning rate, default is 0 for voc.')
    parser.add_argument('--lr-warmup-factor', type=float, default=(1.0 / 3.0), help='warmup factor of base lr.')
    parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum, default is 0.9')
    parser.add_argument('--wd', type=str, default='', help='Weight decay, default is 5e-4 for voc')
    parser.add_argument('--log-interval', type=int, default=100, help='Logging mini-batch interval. Default is 100.')
    parser.add_argument('--save-prefix', type=str, default='', help='Saving parameter prefix')
    parser.add_argument('--save-interval', type=int, default=1, help='Saving parameters epoch interval, best model will always be saved.')
    parser.add_argument('--val-interval', type=int, default=1, help='Epoch interval for validation, increase the number will reduce the training time if validation is slow.')
    parser.add_argument('--seed', type=int, default=233, help='Random seed to be fixed.')
    parser.add_argument('--verbose', dest='verbose', action='store_true', help='Print helpful debugging info once set.')
    parser.add_argument('--mixup', action='store_true', help='Use mixup training.')
    parser.add_argument('--no-mixup-epochs', type=int, default=20, help='Disable mixup training if enabled in the last N epochs.')
    parser.add_argument('--norm-layer', type=str, default=None, help="Type of normalization layer to use. If set to None, backbone normalization layer will be fixed, and no normalization layer will be used. Currently supports 'bn', and None, default is None.Note that if horovod is enabled, sync bn will not work correctly.")
    parser.add_argument('--use-fpn', action='store_true', help='Whether to use feature pyramid network.')
    parser.add_argument('--disable-hybridization', action='store_true', help='Whether to disable hybridize the model. Memory usage and speed will decrese.')
    parser.add_argument('--static-alloc', action='store_true', help='Whether to use static memory allocation. Memory usage will increase.')
    parser.add_argument('--amp', action='store_true', help='Use MXNet AMP for mixed precision training.')
    parser.add_argument('--horovod', action='store_true', help='Use MXNet Horovod for distributed training. Must be run with OpenMPI. --gpus is ignored when using --horovod.')
    parser.add_argument('--executor-threads', type=int, default=1, help='Number of threads for executor for scheduling ops. More threads may incur higher GPU memory footprint, but may speed up throughput. Note that when horovod is used, it is set to 1.')
    parser.add_argument('--kv-store', type=str, default='nccl', help='KV store options. local, device, nccl, dist_sync, dist_device_sync, dist_async are available.')
    args = parser.parse_args()
    if args.horovod:
        if (hvd is None):
            raise SystemExit('Horovod not found, please check if you installed it correctly.')
        hvd.init()
    if (args.dataset == 'voc'):
        args.epochs = (int(args.epochs) if args.epochs else 20)
        args.lr_decay_epoch = (args.lr_decay_epoch if args.lr_decay_epoch else '14,20')
        args.lr = (float(args.lr) if args.lr else 0.001)
        args.lr_warmup = (args.lr_warmup if args.lr_warmup else (- 1))
        args.wd = (float(args.wd) if args.wd else 0.0005)
    elif (args.dataset == 'coco'):
        args.epochs = (int(args.epochs) if args.epochs else 26)
        args.lr_decay_epoch = (args.lr_decay_epoch if args.lr_decay_epoch else '17,23')
        args.lr = (float(args.lr) if args.lr else 0.01)
        args.lr_warmup = (args.lr_warmup if args.lr_warmup else 1000)
        args.wd = (float(args.wd) if args.wd else 0.0001)
    return args