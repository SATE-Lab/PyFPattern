def cross_entropy(input, target, weight=None, size_average=True, ignore_index=(- 100), reduce=True):
    'This criterion combines `log_softmax` and `nll_loss` in a single\n    function.\n\n    See :class:`torch.nn.CrossEntropyLoss` for details.\n\n    Args:\n        input: Variable :math:`(N, C)` where `C = number of classes`\n        target: Variable :math:`(N)` where each value is\n            `0 <= targets[i] <= C-1`\n        weight (Tensor, optional): a manual rescaling weight given to each\n                class. If given, has to be a Tensor of size "nclasses"\n        size_average (bool, optional): By default, the losses are averaged\n                over observations for each minibatch. However, if the field\n                sizeAverage is set to False, the losses are instead summed\n                for each minibatch. Ignored if reduce is False. Default: True\n        ignore_index (int, optional): Specifies a target value that is ignored\n                and does not contribute to the input gradient. When size_average is\n                True, the loss is averaged over non-ignored targets. Default: -100\n        reduce (bool, optional): By default, the losses are averaged or summed over\n                observations for each minibatch depending on size_average. When reduce\n                is False, returns a loss per batch element instead and ignores\n                size_average. Default: True\n\n    Examples::\n\n        >>> input = autograd.Variable(torch.randn(3, 5), requires_grad=True)\n        >>> target = autograd.Variable(torch.LongTensor(3).random_(5))\n        >>> loss = F.cross_entropy(input, target)\n        >>> loss.backward()\n    '
    return nll_loss(log_softmax(input, 1), target, weight, size_average, ignore_index, reduce)