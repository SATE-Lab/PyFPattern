def create_train_op(total_loss, optimizer, global_step=_USE_GLOBAL_STEP, update_ops=None, variables_to_train=None, clip_gradient_norm=0, summarize_gradients=False, gate_gradients=tf_optimizer.Optimizer.GATE_OP, aggregation_method=None, colocate_gradients_with_ops=False, gradient_multipliers=None, check_numerics=True):
    "Creates an `Operation` that evaluates the gradients and returns the loss.\n\n  Args:\n    total_loss: A `Tensor` representing the total loss.\n    optimizer: A tf.Optimizer to use for computing the gradients.\n    global_step: A `Tensor` representing the global step variable. If left as\n      `_USE_GLOBAL_STEP`, then slim.variables.global_step() is used.\n    update_ops: An optional list of updates to execute. If `update_ops` is\n      `None`, then the update ops are set to the contents of the\n      `tf.GraphKeys.UPDATE_OPS` collection. If `update_ops` is not `None`, but\n      it doesn't contain all of the update ops in `tf.GraphKeys.UPDATE_OPS`,\n      a warning will be displayed.\n    variables_to_train: an optional list of variables to train. If None, it will\n      default to all tf.trainable_variables().\n    clip_gradient_norm: If greater than 0 then the gradients would be clipped\n      by it.\n    summarize_gradients: Whether or not add summaries for each gradient.\n    gate_gradients: How to gate the computation of gradients. See tf.Optimizer.\n    aggregation_method: Specifies the method used to combine gradient terms.\n      Valid values are defined in the class `AggregationMethod`.\n    colocate_gradients_with_ops: Whether or not to try colocating the gradients\n      with the ops that generated them.\n    gradient_multipliers: A dictionary of either `Variables` or `Variable` op\n      names to the coefficient by which the associated gradient should be\n      scaled.\n    check_numerics: Whether or not we apply check_numerics.\n\n  Returns:\n    A `Tensor` that when evaluated, computes the gradients and returns the total\n      loss value.\n  "

    def transform_grads_fn(grads):
        if gradient_multipliers:
            with ops.name_scope('multiply_grads'):
                grads = multiply_gradients(grads, gradient_multipliers)
        if (clip_gradient_norm > 0):
            with ops.name_scope('clip_grads'):
                grads = clip_gradient_norms(grads, clip_gradient_norm)
        return grads
    return training.create_train_op(total_loss=total_loss, optimizer=optimizer, global_step=global_step, update_ops=update_ops, variables_to_train=variables_to_train, transform_grads_fn=transform_grads_fn, summarize_gradients=summarize_gradients, gate_gradients=gate_gradients, aggregation_method=aggregation_method, colocate_gradients_with_ops=colocate_gradients_with_ops, check_numerics=check_numerics)