

@templatedoc()
def py_func(func, x, out, backward_func=None, skip_vars_in_backward_input=None):
    '\n    This OP is used to register customized Python OP to Paddle Fluid. The design \n    principe of py_func is that LodTensor and numpy array can be converted to each\n    other easily. So you can use Python and numpy API to register a python OP.\n\n    The forward  function of the registered OP is ``func`` and the backward function \n    of that is  ``backward_func``. Paddle will call ``func`` at forward runtime and \n    call ``backward_func`` at backward runtime(if ``backward_func`` is not  None). \n    ``x`` is the input of ``func``, whose type must be LoDTensor; ``out`` is \n    the output of ``func``, whose type can be either LoDTensor or numpy array.\n\n    The input of the backward function ``backward_func`` is ``x``, ``out`` and \n    the gradient of ``out``. If some variables of ``out`` have no gradient, the \n    relevant input variable of ``backward_func`` is None. If some variables of \n    ``x`` do not have a gradient, the user should return None in ``backward_func``.\n\n    The data type and shape of ``out`` should also be set correctly before this \n    API is called, and the data type and shape of the gradient of ``out`` and \n    ``x`` will be inferred automatically.\n\n    This API can also be used to debug the neural network by setting the ``func``\n    as a function that only print variables.\n\n    Args:\n        func (callable): The forward function of the registered OP. When the network\n            is running, the forward output ``out`` will be calculated according to this \n            function and the forward input ``x``. In ``func`` , it\'s suggested that we \n            actively convert LoDTensor into a numpy array, so that we can use Python and\n            numpy API arbitrarily. If not, some operations of numpy may not be compatible.\n        x (Variable|tuple(Variale)|list[Variale]): The input of the forward function ``func``. \n            It can be Variable|tuple(Variale)|list[Variale], where Variable is LoDTensor or \n            Tenosor. In addition, Multiple Variable should be passed in the form of tuple(Variale)\n            or list[Variale].\n        out (Variable|tuple(Variale)|list[Variale]): The output of the forward function ``func``, \n            it can be Variable|tuple(Variale)|list[Variale], where Variable can be either LoDTensor\n            or numpy array. Since Paddle cannot automatically infer the shape and type of ``out``, \n            you must create ``out`` in advance.\n        backward_func (callable, optional): The backward function of the registered OP. \n            Its default value is None, which means there is no reverse calculation. If \n            it is not None, ``backward_func`` is called to calculate the gradient of \n            ``x`` when the network is at backward runtime.\n        skip_vars_in_backward_input (Variable, optional): It\'s used to limit the input \n            variable list of ``backward_func``, and it can be Variable|tuple(Variale)|list[Variale]. \n            It must belong to either ``x`` or ``out``. The default  value is None, which means \n            that no variables need to be removed from ``x`` and ``out``. If it is not None, \n            these variables will not be the input of ``backward_func``. This parameter is only \n            useful when ``backward_func`` is not None.\n    \n    Returns: \n        Variable|tuple(Variale)|list[Variale]: The output ``out`` of the forward function ``func``.\n\n    Examples:\n        .. code-block:: python\n\t    \n            # example 1:\n            import paddle.fluid as fluid\n            import six\n\n            # Creates a forward function, LodTensor can be input directly without\n            # being converted into numpy array.\n            def tanh(x):\n                return np.tanh(x)\n\n            # Skip x in backward function and return the gradient of x\n            # LodTensor must be actively converted to numpy array, otherwise, \n            # operations such as +/- can\'t be used.\n            def tanh_grad(y, dy):\n                return np.array(dy) * (1 - np.square(np.array(y)))\n            \n            # Creates a forward function for debugging running networks(print value)\n            def debug_func(x):\n                print(x)\n            \n            def create_tmp_var(name, dtype, shape):\n                return fluid.default_main_program().current_block().create_var(\n                    name=name, dtype=dtype, shape=shape)\n\n            def simple_net(img, label):\n                hidden = img\n                for idx in six.moves.range(4):\n                    hidden = fluid.layers.fc(hidden, size=200)\n                    new_hidden = create_tmp_var(name=\'hidden_{}\'.format(idx),\n                        dtype=hidden.dtype, shape=hidden.shape)\n\n                    # User-defined forward and backward \n                    hidden = fluid.layers.py_func(func=tanh, x=hidden,\n                        out=new_hidden, backward_func=tanh_grad,\n                        skip_vars_in_backward_input=hidden)\n\n                    # User-defined debug functions that print out the input LodTensor\n                    fluid.layers.py_func(func=debug_func, x=hidden, out=None)\n\n                prediction = fluid.layers.fc(hidden, size=10, act=\'softmax\')\n                loss = fluid.layers.cross_entropy(input=prediction, label=label)\n                return fluid.layers.mean(loss)\n\n            # example 2: \n            # This example shows how to turn LoDTensor into numpy array and \n            # use numpy API to register an Python OP\n            import paddle.fluid as fluid\n            import numpy as np\n\n            def element_wise_add(x, y): \n                # LodTensor must be actively converted to numpy array, otherwise, \n                # numpy.shape can\'t be used.\n                x = np.array(x)    \n                y = np.array(y)\n\n                if x.shape != y.shape:\n                    raise AssertionError("the shape of inputs must be the same!")\n\n                result = np.zeros(x.shape, dtype=\'int32\')\n                for i in range(len(x)):\n                    for j in range(len(x[0])):\n                        result[i][j] = x[i][j] + y[i][j]\n\n                return result\n\n            def create_tmp_var(name, dtype, shape):\n                return fluid.default_main_program().current_block().create_var(\n                            name=name, dtype=dtype, shape=shape)\n\n            def py_func_demo():\n                start_program = fluid.default_startup_program()\n                main_program = fluid.default_main_program()\n\n                # Input of the forward function\n                x = fluid.data(name=\'x\', shape=[2,3], dtype=\'int32\')\n                y = fluid.data(name=\'y\', shape=[2,3], dtype=\'int32\')\n                \n                # Output of the forward function, name/dtype/shape must be specified\n                output = create_tmp_var(\'output\',\'int32\', [3,1])\n\n                # Multiple Variable should be passed in the form of tuple(Variale) or list[Variale]\n                fluid.layers.py_func(func=element_wise_add, x=[x,y], out=output)\n\n                exe=fluid.Executor(fluid.CPUPlace())\n                exe.run(start_program)\n\n                # Feed numpy array to main_program\n                input1 = np.random.randint(1, 10, size=[2,3], dtype=\'int32\')\n                input2 = np.random.randint(1, 10, size=[2,3], dtype=\'int32\')\n                out = exe.run(main_program, \n                            feed={\'x\':input1, \'y\':input2},\n                            fetch_list=[output.name])\n                print("{0} + {1} = {2}".format(input1, input2, out))\n\n            py_func_demo()\n\n            # Reference output:\n            # [[5, 9, 9]   + [[7, 8, 4]  =  [array([[12, 17, 13]\n            #  [7, 5, 2]]     [1, 3, 3]]            [8, 8, 5]], dtype=int32)]\n    '
    helper = LayerHelper('py_func', **locals())
    if (x is None):
        x = []
    elif isinstance(x, Variable):
        x = [x]
    elif isinstance(x, tuple):
        x = list(x)
    elif (not isinstance(x, (list, tuple, Variable))):
        raise TypeError('Input must be Variable/list(Variable)/tuple(Variable)')
    if (out is None):
        out_list = []
    elif isinstance(out, Variable):
        out_list = [out]
    elif isinstance(out, tuple):
        out_list = list(out)
    elif (not isinstance(x, (list, tuple, Variable))):
        raise TypeError('Output must be Variable/list(Variable)/tuple(Variable)')
    fwd_func_id = PyFuncRegistry(func).id
    bwd_func_id = (PyFuncRegistry(backward_func).id if (backward_func is not None) else (- 1))
    for each_out in out_list:
        if (len(each_out.shape) == 0):
            raise ValueError('Output shapes of py_func op should be provided by users manually')
    backward_skip_vars = set()
    if ((backward_func is not None) and (skip_vars_in_backward_input is not None)):
        if isinstance(skip_vars_in_backward_input, Variable):
            skip_vars_in_backward_input = [skip_vars_in_backward_input]
        fwd_in_out = [v.name for v in x]
        fwd_in_out.extend([v.name for v in out_list])
        fwd_in_out = set(fwd_in_out)
        backward_skip_vars = set()
        for v in skip_vars_in_backward_input:
            if (not (v.name in fwd_in_out)):
                raise ValueError('Variable {} is not found in forward inputs and outputs'.format(v.name))
            backward_skip_vars.add(v.name)
    helper.append_op(type='py_func', inputs={
        'X': x,
    }, outputs={
        'Out': out_list,
    }, attrs={
        'forward_callable_id': fwd_func_id,
        'backward_callable_id': bwd_func_id,
        'backward_skip_vars': list(backward_skip_vars),
    })
    return out
