def embedding_rnn_decoder(decoder_inputs, initial_state, cell, num_symbols, embedding_size, output_projection=None, feed_previous=False, update_embedding_for_previous=True, scope=None):
    'RNN decoder with embedding and a pure-decoding option.\n\n  Args:\n    decoder_inputs: A list of 1D batch-sized int32 Tensors (decoder inputs).\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    cell: rnn_cell.RNNCell defining the cell function.\n    num_symbols: Integer, how many symbols come into the embedding.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_symbols] and B has\n      shape [num_symbols]; if provided and feed_previous=True, each fed\n      previous output will first be multiplied by W and added B.\n    feed_previous: Boolean; if True, only the first of decoder_inputs will be\n      used (the "GO" symbol), and all other decoder inputs will be generated by:\n        next = embedding_lookup(embedding, argmax(previous_output)),\n      In effect, this implements a greedy decoder. It can also be used\n      during training to emulate http://arxiv.org/abs/1506.03099.\n      If False, decoder_inputs are used as given (the standard decoder case).\n    update_embedding_for_previous: Boolean; if False and feed_previous=True,\n      only the embedding for the first symbol of decoder_inputs (the "GO"\n      symbol) will be updated by back propagation. Embeddings for the symbols\n      generated from the decoder itself remain unchanged. This parameter has\n      no effect if feed_previous=False.\n    scope: VariableScope for the created subgraph; defaults to\n      "embedding_rnn_decoder".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors. The\n        output is of shape [batch_size x cell.output_size] when\n        output_projection is not None (and represents the dense representation\n        of predicted tokens). It is of shape [batch_size x num_decoder_symbols]\n        when output_projection is None.\n      state: The state of each decoder cell in each time-step. This is a list\n        with length len(decoder_inputs) -- one item for each time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: When output_projection has the wrong shape.\n  '
    with variable_scope.variable_scope((scope or 'embedding_rnn_decoder')) as scope:
        if (output_projection is not None):
            dtype = scope.dtype
            proj_weights = ops.convert_to_tensor(output_projection[0], dtype=dtype)
            proj_weights.get_shape().assert_is_compatible_with([None, num_symbols])
            proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)
            proj_biases.get_shape().assert_is_compatible_with([num_symbols])
        embedding = variable_scope.get_variable('embedding', [num_symbols, embedding_size])
        loop_function = (_extract_argmax_and_embed(embedding, output_projection, update_embedding_for_previous) if feed_previous else None)
        emb_inp = (embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs)
        return rnn_decoder(emb_inp, initial_state, cell, loop_function=loop_function)