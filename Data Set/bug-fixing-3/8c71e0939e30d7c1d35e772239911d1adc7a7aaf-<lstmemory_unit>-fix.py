@wrap_name_default('lstm_unit')
def lstmemory_unit(input, out_memory=None, name=None, size=None, param_attr=None, act=None, gate_act=None, state_act=None, input_proj_bias_attr=None, input_proj_layer_attr=None, lstm_bias_attr=None, lstm_layer_attr=None):
    '\n    lstmemory_unit defines the caculation process of a LSTM unit during a \n    single time step. This function is not a recurrent layer, so it can not be\n    directly used to process sequence input. This function is always used in\n    recurrent_group (see layers.py for more details) to implement attention\n    mechanism.\n\n    Please refer to  **Generating Sequences With Recurrent Neural Networks**\n    for more details about LSTM. The link goes as follows:\n    .. _Link: https://arxiv.org/abs/1308.0850\n\n    ..  math::\n\n        i_t & = \\sigma(W_{x_i}x_{t} + W_{h_i}h_{t-1} + W_{c_i}c_{t-1} + b_i)\n\n        f_t & = \\sigma(W_{x_f}x_{t} + W_{h_f}h_{t-1} + W_{c_f}c_{t-1} + b_f)\n\n        c_t & = f_tc_{t-1} + i_t tanh (W_{x_c}x_t+W_{h_c}h_{t-1} + b_c)\n\n        o_t & = \\sigma(W_{x_o}x_{t} + W_{h_o}h_{t-1} + W_{c_o}c_t + b_o)\n\n        h_t & = o_t tanh(c_t)\n\n    The example usage is:\n\n    ..  code-block:: python\n\n        lstm_step = lstmemory_unit(input=[layer1],\n                                   size=256,\n                                   act=TanhActivation(),\n                                   gate_act=SigmoidActivation(),\n                                   state_act=TanhActivation())\n\n\n    :param input: input layer.\n    :type input: LayerOutput\n    :param out_memory: output of previous time step\n    :type out_memory: LayerOutput | None\n    :param name: lstmemory unit name.\n    :type name: basestring\n    :param size: lstmemory unit size.\n    :type size: int\n    :param param_attr: parameter attribute, None means default attribute.\n    :type param_attr: ParameterAttribute\n    :param act: last activiation type of lstm.\n    :type act: BaseActivation\n    :param gate_act: gate activiation type of lstm.\n    :type gate_act: BaseActivation\n    :param state_act: state activiation type of lstm.\n    :type state_act: BaseActivation\n    :param input_proj_bias_attr: bias attribute for input to hidden projection.\n                False means no bias, None means default bias.\n    :type input_proj_bias_attr: ParameterAttribute|False|None\n    :param input_proj_layer_attr: extra layer attribute for input to hidden\n                projection of the LSTM unit, such as dropout, error clipping.\n    :type input_proj_layer_attr: ExtraLayerAttribute\n    :param lstm_bias_attr: bias parameter attribute of lstm layer.\n                False means no bias, None means default bias.\n    :type lstm_bias_attr: ParameterAttribute|False|None\n    :param lstm_layer_attr: extra attribute of lstm layer.\n    :type lstm_layer_attr: ExtraLayerAttribute\n    :return: lstmemory unit name.\n    :rtype: LayerOutput\n    '
    if (size is None):
        assert ((input.size % 4) == 0)
        size = (input.size / 4)
    if (out_memory is None):
        out_mem = memory(name=name, size=size)
    else:
        out_mem = out_memory
    state_mem = memory(name=('%s_state' % name), size=size)
    with mixed_layer(name=('%s_input_recurrent' % name), size=(size * 4), bias_attr=input_proj_bias_attr, layer_attr=input_proj_layer_attr, act=IdentityActivation()) as m:
        m += identity_projection(input=input)
        m += full_matrix_projection(input=out_mem, param_attr=param_attr)
    lstm_out = lstm_step_layer(name=name, input=m, state=state_mem, size=size, bias_attr=lstm_bias_attr, act=act, gate_act=gate_act, state_act=state_act, layer_attr=lstm_layer_attr)
    get_output_layer(name=('%s_state' % name), input=lstm_out, arg_name='state')
    return lstm_out